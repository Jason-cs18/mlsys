---
title: Reterival Augmented Generation (RAG)
date: '2024-11-17'
tags: ['deep learning', 'LLM applications']
draft: false
summary: 'Empowering large language models with external knowledge.'
---

Large language models (LLMs) are good at leveraging their learned knowledge to complete tasks. However, their knowledge is obtained from costly training and is often outdated. To address this issue, we can use external knowledge sources to augment the LLMs' knowledge. In this post, we will introduce retrieval augmented generation (RAG), a training-free approach to augment models' knowledge.

<details>
<summary><b>Table of Contents</b> (click to open)</summary>

- [RAG](#rag)
- [Building a question-answering system with RAG](#building-a-question-answering-system-with-rag)
- [Learning to ask via Agentic RAG](#learning-to-ask-via-agentic-rag)
- [Challenges for RAG](#challenges-for-rag)
- [References](#references)

</details>

## RAG

RAG is an architectural approach to improve the efficacy of LLM applications with custom data. In specific, it reterivals data/documents relevant to a question or task and providing them as context for the LLM. RAG is an effective approach to enable LLMs maintain up-to-date external knowledge.

![](https://www.databricks.com/sites/default/files/inline-images/glossary-rag-image-2.png?v=1704903053)

_image source: [What Is Retrieval Augmented Generation, or RAG?](https://www.databricks.com/glossary/retrieval-augmented-generation-rag)_

## Building a question-answering system with RAG
As illustrated above, RAG consists of 4 steps: data preparation, index relevant data, information retrieval, and LLM inference. In this section, we will use LlamaIndex to build a simple question-answering system with RAG. LlamaIndex is a framework for building context-augmented LLM applications.

## Learning to ask via Agentic RAG
A simple RAG depends on the searched context. However, the context is not always the best answer. To address this issue, we can use Agentic RAG to improve the performance of RAG. Agentic RAG is a framework that enables LLMs to reformulate their queries and self-query to improve the performance of RAG.

## Challenges for RAG
Although RAG is a powerful approach to augment LLMs' knowledge, it also has some challenges. For example, **the limited context size** is not enough to generate a full tech report. 


## References
1. [What Is Retrieval Augmented Generation, or RAG?](https://www.databricks.com/glossary/retrieval-augmented-generation-rag) 
2. [LIamaIndex: Build AI Knowledge Assistants over your enterprise data](https://www.llamaindex.ai/framework)
    - [Tutorial: Building a RAG pipeline](https://docs.llamaindex.ai/en/stable/understanding/rag/)
    - [Tutorial: Building an agent](https://docs.llamaindex.ai/en/stable/understanding/agent/)
    - [Video: Building and deploying multi-agent RAG systems in 2024 with LlamaIndex](https://www.youtube.com/watch?v=lqRTCxsKBwc)
3. [Agentic RAG: turbocharge your RAG with query reformulation and self-query!](https://huggingface.co/learn/cookbook/agent_rag)