---
title: On-Device Large Language Models (Part 2)
date: '2024-11-26'
tags: ['LLM', 'MLSys', 'on-device']
draft: false
summary: 'Recent advances for on-device LLM inference strategies'
---
In [Part 1](https://jason-cs18.github.io/mlsys/blog/mlsys/on_device_llm), we explored efficient system design for on-device LLM inference. Now, in Part 2, we delve into the algorithmic design aspect of on-device LLM inference. To begin, we introduce the inference scaling law, followed by an overview of the primary scaling techniques.

## Inference scaling law

### Model scaling

### Token scaling

## References
1. Miao et al. ["Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"](https://arxiv.org/abs/2312.15234) arXiv preprint arXiv:2312.15234 (2023).
2. Xu et al. ["On-Device Language Models: A Comprehensive Review"](https://arxiv.org/pdf/2409.00088) arXiv preprint arXiv:2409.00088 (2024).
3. Dettmers et al. ["The case for 4-bit precision: k-bit Inference Scaling Laws"](https://openreview.net/pdf?id=i8tGb1ab1j) ICML 2023.
4. Wu et al. ["Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving"](https://arxiv.org/html/2408.00724v2) arXiv preprint arXiv:2408.00724 (2024).
5. Snell et al. ["Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"](https://arxiv.org/pdf/2408.03314) arXiv preprint arXiv:2408.03314 (2024).
6. Chen et al. ["Are More LM Calls All You Need? Towards the Scaling Properties of Compound AI Systems"](https://arxiv.org/pdf/2403.02419) arXiv preprint arXiv:2403.02419 (2024).
7. Brown et al. ["Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"](https://arxiv.org/pdf/2407.21787) arXiv preprint arXiv:2407.21787 (2024).