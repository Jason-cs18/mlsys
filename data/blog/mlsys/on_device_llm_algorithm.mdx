---
title: On-Device Large Language Models (Part 2)
date: '2024-11-26'
tags: ['LLM', 'MLSys']
draft: false
summary: 'Recent advances for on-device LLM inference strategies'
---
In [Part 1](https://jason-cs18.github.io/mlsys/blog/mlsys/on_device_llm), we explored efficient system design for on-device LLM inference. Now, in Part 2, we delve into the algorithmic design aspect of on-device LLM inference. To begin, we introduce the inference scaling law, followed by an overview of the primary scaling techniques.

<details>
<summary><b>Table of Contents</b> (click to open)</summary>

- [Inference scaling law](#inference-scaling-law)
  - [Model scaling](#model-scaling)
  - [Token scaling](#token-scaling)
- [Use inference scaling law in retrieve-augmented generation](#use-inference-scaling-law-in-retrieve-augmented-generation)
  - [More effective retrieved documents](#more-effective-retrieved-documents)
  - [More advanced inference strategies](#more-advanced-inference-strategies)
- [References](#references)

</details>

## Inference scaling law
In addition to increasing training compute, many companies discover that scaling up inference compute is a cost-effective method for enhancing the performance of LLMs. For instance, the performance of [OpenAI's o1](https://en.wikipedia.org/wiki/OpenAI_o1) consistently improves with increased test-time compute. The inference scaling law ([Wu et al.](https://arxiv.org/html/2408.00724v2)) can be expressed as follows:

$$
Accuracy  \varpropto f(N, T_{s})
$$
where $N$ denotes the number of model parameters, and $T_{s}$ denotes the number of tokens using the inference strategy $s$.

### Model scaling

### Token scaling

## Use inference scaling law in retrieve-augmented generation
Retrieve-augmented generation (RAG) differs from general LLM inference in that it follows a two-stage process. In the first stage, relevant documents are retrieved from a corpus, while in the second stage, a response is generated based on the retrieved documents. Unlike general LLMs, the model size of RAG remains fixed. The inference accuracy is determined by the number of effective retrieved documents and the choice of inference strategies.
As a result, the inference scaling law of RAG ([Yue et al.](https://arxiv.org/pdf/2410.04343)) can be formulated as follows:
$$
Accuracy  \varpropto f(C, T_{s}) 
$$
where $C$ denotes the number of effective retrieved documents, and $T_{s}$ denotes the number of tokens using the inference strategy $s$.

### More effective retrieved documents
xxx

### More advanced inference strategies
xxx

## References
1. Miao et al. ["Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"](https://arxiv.org/abs/2312.15234) arXiv preprint arXiv:2312.15234 (2023).
2. Xu et al. ["On-Device Language Models: A Comprehensive Review"](https://arxiv.org/pdf/2409.00088) arXiv preprint arXiv:2409.00088 (2024).
3. Dettmers et al. ["The case for 4-bit precision: k-bit Inference Scaling Laws"](https://openreview.net/pdf?id=i8tGb1ab1j) ICML 2023.
4. Wu et al. ["Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving"](https://arxiv.org/html/2408.00724v2) arXiv preprint arXiv:2408.00724 (2024).
5. Snell et al. ["Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"](https://arxiv.org/pdf/2408.03314) arXiv preprint arXiv:2408.03314 (2024).
6. Chen et al. ["Are More LM Calls All You Need? Towards the Scaling Properties of Compound AI Systems"](https://arxiv.org/pdf/2403.02419) arXiv preprint arXiv:2403.02419 (2024).
7. Brown et al. ["Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"](https://arxiv.org/pdf/2407.21787) arXiv preprint arXiv:2407.21787 (2024).
8. Yue et al. ["Inference Scaling for Long-Context Retrieval Augmented Generation"](https://arxiv.org/pdf/2410.04343) arXiv preprint arXiv:2410.04343 (2024).