---
title: AI Accelerators and Inference Engine
date: '2024-11-15'
tags: ['deep learning', 'system']
draft: false
summary: 'An overview of AI accelerators and inference engine'
---
Running compute/memory-intensive deep learning models is not easy. To reduce the latency of inference, many efforts have been made from hardware and software perspectives.

<details>
<summary><b>Table of Contents</b> (click to open)</summary>

- [AI Accelerators](#ai-accelerators)
- [AI Compilers](#ai-compilers)
- [Inference Engine](#inference-engine)
- [References](#references)

</details>

## AI Accelerators

AI芯片需求：
- 推理：xxx
- 训练：xxx

传统的计算机架构可以分为SISD,SIMD,MISD,MIMD。
- SISD (Single Instruction, Single Data)
- SIMD (Single Instruction, Multiple Data)
- MISD (Multiple Instruction, Single Data)
- MIMD (Multiple Instruction, Multiple Data)

NVIDIA GPU

Tensor Core, NVLink, and NVSwitch

## AI Compilers

## Inference Engine

## References
1. [AI 硬件体系架构](https://chenzomi12.github.io/02Hardware/README.html)
2. [AI 编程与编译原理](https://chenzomi12.github.io/03Compiler/README.html)
3. [推理系统&引擎](https://chenzomi12.github.io/04Inference/README.html) 