---
title: AI Accelerators and Inference Engine
date: '2024-11-21'
tags: ['deep learning', 'system']
draft: false
summary: 'An overview of AI accelerators and inference engine'
---
Running compute/memory-intensive deep learning models is not easy. To reduce the latency of inference, many efforts have been made from hardware and software perspectives.

<details>
<summary><b>Table of Contents</b> (click to open)</summary>

- [AI Accelerators](#ai-accelerators)
- [AI Compilers](#ai-compilers)
- [Inference Engine](#inference-engine)
- [References](#references)

</details>

## AI Accelerators

AI芯片需求：
- 推理：quantization, sparse, low power
- 训练：high memory bandwidth, mixed precision

传统的计算机架构可以分为SISD,SIMD,MISD,MIMD。
- SISD (Single Instruction, Single Data)
- SIMD (Single Instruction, Multiple Data)
- MISD (Multiple Instruction, Single Data)
- MIMD (Multiple Instruction, Multiple Data)

NVIDIA GPU
- GPU基本工作原理：GPU架构（存储和计算），GPU如何利用线程进行并行计算。
- 为什么GPU适合AI：结合计算强度，理解GPU高吞吐架构对AI计算的影响。
- GPU架构回顾：`#SM`越来越多，计算能力越来越强（搭配最新TensorCore），内存带宽越来越快（搭配最新NVLink）。

![](https://chenzomi12.github.io/_images/01introduction02.png)
_[Single-Instruction Multiple-Data (SIMD) /Single-Instruction Multiple-Thread (SIMT)](https://chenzomi12.github.io/02Hardware07Thought/01Introduction.html)_

![](https://chenzomi12.github.io/_images/03SPMT10.png)
_[SISD/SIMD/SIMT on `C[i]=A[i]+B[i]`](https://chenzomi12.github.io/02Hardware07Thought/03SPMT.html)_

Tensor Core, NVLink, and NVSwitch

![](https://chenzomi12.github.io/_images/04BasicNvlink04.png)
_[集合通信原语](https://chenzomi12.github.io/02Hardware04NVIDIA/04BasicNvlink.html)_

![](https://chenzomi12.github.io/_images/04BasicNvlink02.png)
_[通信硬件](https://chenzomi12.github.io/02Hardware04NVIDIA/04BasicNvlink.html)_

![](https://chenzomi12.github.io/_images/06DeepNvswitch09.png)
_[NVSwitch has better scaling](https://chenzomi12.github.io/02Hardware04NVIDIA/06DeepNvswitch.html)_

- Tensor Core
    - The architecture of Tensor Core
    - How to use Tensor Core for AI acceleration？ 
    - How to use Tensor Core in practice？
- NVLink (多机多卡)
    - The architecture of NVLink
    - How to use NVLink for AI acceleration？
    - How to use NVLink in practice？
- NVSwitch (单机多卡)
    - The architecture of NVSwitch
    - How to use NVSwitch for AI acceleration？
    - How to use NVSwitch in practice？

## AI Compilers

## Inference Engine

## References
1. [AI 硬件体系架构](https://chenzomi12.github.io/02Hardware/README.html)
2. [AI 编程与编译原理](https://chenzomi12.github.io/03Compiler/README.html)
3. [推理系统&引擎](https://chenzomi12.github.io/04Inference/README.html) 