---
title: AI Accelerators and Inference Engine
date: '2024-11-15'
tags: ['deep learning', 'system']
draft: false
summary: 'An overview of AI accelerators and inference engine'
---
Running compute/memory-intensive deep learning models is not easy. To reduce the latency of inference, many efforts have been made from hardware and software perspectives.

<details>
<summary><b>Table of Contents</b> (click to open)</summary>

- [AI Accelerators](#ai-accelerators)
- [AI Compilers](#ai-compilers)
- [Inference Engine](#inference-engine)
- [References](#references)

</details>

## AI Accelerators

AI芯片需求：
- 推理：quantization, sparse, low power
- 训练：high memory bandwidth, mixed precision

传统的计算机架构可以分为SISD,SIMD,MISD,MIMD。
- SISD (Single Instruction, Single Data)
- SIMD (Single Instruction, Multiple Data)
- MISD (Multiple Instruction, Single Data)
- MIMD (Multiple Instruction, Multiple Data)

NVIDIA GPU
- GPU基本工作原理：GPU架构（存储和计算），GPU如何利用线程进行并行计算。
- 为什么GPU适合AI：结合计算强度，理解GPU高吞吐架构对AI计算的影响。
- GPU架构回顾：`#SM`越来越多，计算能力越来越强（搭配最新TensorCore），内存带宽越来越快（搭配最新NVLink）。

Tensor Core, NVLink, and NVSwitch

- Tensor Core
    - The architecture of Tensor Core
    - How to use Tensor Core for AI acceleration？ 
    - How to use Tensor Core in practice？
- NVLink
    - The architecture of NVLink
    - How to use NVLink for AI acceleration？
    - How to use NVLink in practice？

## AI Compilers

## Inference Engine

## References
1. [AI 硬件体系架构](https://chenzomi12.github.io/02Hardware/README.html)
2. [AI 编程与编译原理](https://chenzomi12.github.io/03Compiler/README.html)
3. [推理系统&引擎](https://chenzomi12.github.io/04Inference/README.html) 