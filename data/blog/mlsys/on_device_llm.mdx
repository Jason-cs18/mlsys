---
title: On-Device Large Language Models (Part 1)
date: '2024-11-25'
tags: ['LLM', 'MLSys']
draft: false
summary: 'Recent advances for on-device LLM inference system'
---
Deploying large language models (LLMs) on devices is gaining significant attention due to the increasing demand for accessibility and privacy. This field is seeing two main trends: one involves creating smaller versions of LLMs, while the other focuses on designing efficient systems to run LLMs on devices.

In this post, we will concentrate on the second trend and present some recent and interesting works related to on-device LLM inference.

<details>
<summary><b>Table of Contents</b> (click to open)</summary>

- [How to run LLMs?](#how-to-run-llms)
- [On-device LLM inference](#on-device-llm-inference)
- [Open-source inference engines](#open-source-inference-engines)
- [Examples for llama.cpp](#examples-for-llama-.-cpp)
- [References](#references)

</details>

## How to run LLMs?

On NVIDIA GPU, how to run a LLM?

## On-device LLM inference
challenges - representative works - insights

## Open-source inference engines

|Inference engine|Supported devices|Heterogenous computing|`#LLM`|Optimizations|
|:--:|:---:|:---:|:---:|:---:|
|[llama.cpp](https://github.com/ggerganov/llama.cpp)|NVIDIA GPU, Intel CPU, Apple silicon, Android|:---:|:---:|:---:|
|[PowerInfer](https://github.com/SJTU-IPADS/PowerInfer)|:---:|:---:|:---:|:---:|

## Examples for llama.cpp

## Other inference systems
To explore the different design rationales between LLMs and other ML models, we summary recent on-device inference systems as follows.

**CACTUS** ([Rastikerdar et al.](https://dl.acm.org/doi/pdf/10.1145/3643832.3661888)) context-aware DNN inference

**CoActo** ([Bin et al.](https://dl.acm.org/doi/pdf/10.1145/3643832.3661885)) collaborative inference between cloud and edge

**Pantheon** ([Han et al.](https://pantheoninfer.github.io/)) multi-DNN inference

## References
1. Yi et al. ["EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models"](https://arxiv.org/abs/2308.14352) arXiv preprint arXiv:2308.14352 (2023).
2. Song et al. ["PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"](https://dl.acm.org/doi/pdf/10.1145/3694715.3695964) SOSP 2024.
3. Xue et al. ["PowerInfer-2: Fast Large Language Model Inference on a Smartphone"](https://arxiv.org/abs/2406.06282) arXiv preprint arXiv:2406.06282 (2024).
4. ggerganov. ["llama.cpp: LLM inference in C/C++"](https://github.com/ggerganov/llama.cpp) Github repo.
5. SJTU-IPADS. ["PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"](https://github.com/SJTU-IPADS/PowerInfer) Github repo.
6. Rastikerdar et al. ["CACTUS: Dynamically Switchable Context-aware micro-Classifiers for Efficient IoT Inference"](https://dl.acm.org/doi/pdf/10.1145/3643832.3661888) MobiSys 2024.
7. Bin et al. ["CoActo: CoActive Neural Network Inference Oloading with Fine-grained and Concurrent Execution"](https://dl.acm.org/doi/pdf/10.1145/3643832.3661885) MobiSys 2024.
8. Han et al. ["Pantheon: Preemptible Multi-DNN Inference on Mobile Edge GPUs"](https://pantheoninfer.github.io/) MobiSys 2024.