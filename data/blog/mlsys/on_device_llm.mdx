---
title: On-Device Large Language Models
date: '2024-11-25'
tags: ['LLM', 'MLSys', 'on-device']
draft: false
summary: 'Recent advances for on-device LLM inference'
---
Deploying large language models (LLMs) on devices is gaining significant attention due to the increasing demand for accessibility and privacy. This field is seeing two main trends: one involves creating smaller versions of LLMs, while the other focuses on designing efficient systems to run LLMs on devices.

In this post, we will concentrate on the second trend and present some recent and interesting works related to on-device LLM inference.

<details>
<summary><b>Table of Contents</b> (click to open)</summary>

- [How to run LLMs?](#how-to-run-llms)
- [On-device LLM inference](#on-device-llm-inference)
- [Open-source](#open-source)
- [References](#references)

</details>

## How to run LLMs?

On NVIDIA GPU, how to run a LLM?

## On-device LLM inference
challenges - representative works - insights

## Open-source

|Inference engine|Supported devices|Heterogenous computing|`#LLM`|Optimizations|
|:--:|:---:|:---:|:---:|:---:|
|[llama.cpp](https://github.com/ggerganov/llama.cpp)|NVIDIA GPU, Intel CPU, Apple silicon, Android|:---:|:---:|:---:|
|[PowerInfer](https://github.com/SJTU-IPADS/PowerInfer)|:---:|:---:|:---:|:---:|

## References
[1] Yi et al. ["EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models"](https://arxiv.org/abs/2308.14352) arXiv preprint arXiv:2308.14352 (2023).

[2] Song et al. ["PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"](https://dl.acm.org/doi/pdf/10.1145/3694715.3695964) SOSP 2024.

[3] Xue et al. ["PowerInfer-2: Fast Large Language Model Inference on a Smartphone"](https://arxiv.org/abs/2406.06282) arXiv preprint arXiv:2406.06282 (2024).