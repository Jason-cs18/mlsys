---
title: On-Device Diffusion (Part 1)
date: '2024-12-05'
tags: ['Diffusion', 'AIGC']
draft: false
summary: 'Recent advances for on-device diffusion inference algorithms'
---

Diffusion models have been widely adopted in various generation tasks (e.g., avatar synthesis, audio synthesis, etc.). However, the inference of diffusion models is computationally expensive and requires significant computational resources. In this post, we will discuss recent advances for on-device diffusion inference strategies.

Unlike LLMs, the model size of diffusion models is smaller and ranging from 1B to 10B parameters. However, the inference speed is much slower due to computation-intensive operations such as upsampling.